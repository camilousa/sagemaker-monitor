{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3708d1eb-e9f8-4091-b6fd-1533b579da79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import ModelQualityMonitor\n",
    "from sagemaker.model_monitor import EndpointInput\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "\n",
    "import sagemaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6240ce3-7161-44d7-8e3f-da8b92157ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%store -r baseline_dataset_uri\n",
    "%store -r path\n",
    "%store -r endpoint_name\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "baseline_results_uri = f\"s3://{bucket}/{path}/baseline-results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65f58d23-b1b3-44b4-8fd1-8733060d8d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-381492271173/monitor/heart/baseline/dataset/validation_with_predictions.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_dataset_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c86aae03-74b6-4a30-a95a-59654e0ad7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quality_monitor = ModelQualityMonitor(\n",
    "    role='LabRole',\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    sagemaker_session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f0453f-aaa1-4ba5-9d21-8fcfcbbb9646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700e661-0d85-46e0-b610-001bdd0fb6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fae316-dfde-4b0e-b31a-27a4612ca6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e38427f-793a-4762-a359-d3b0c1df8ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2024-03-18-19-19-39-195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................\u001b[34m2024-03-18 19:24:25.382905: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:25.382939: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:26.999741: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:26.999769: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:26.999787: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-138-184.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:27.000054: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:28,556 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:381492271173:processing-job/baseline-suggestion-job-2024-03-18-19-19-39-195', 'ProcessingJobName': 'baseline-suggestion-job-2024-03-18-19-19-39-195', 'Environment': {'analysis_type': 'MODEL_QUALITY', 'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'ground_truth_attribute': 'label', 'inference_attribute': 'prediction', 'output_path': '/opt/ml/processing/output', 'probability_attribute': 'probability', 'problem_type': 'BinaryClassification', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-381492271173/monitor/heart/baseline/dataset/validation_with_predictions.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-381492271173/monitor/heart/baseline-results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::381492271173:role/LabRole', 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}}\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:28,556 - __main__ - INFO - Current Environment:{'analysis_type': 'MODEL_QUALITY', 'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'ground_truth_attribute': 'label', 'inference_attribute': 'prediction', 'output_path': '/opt/ml/processing/output', 'probability_attribute': 'probability', 'problem_type': 'BinaryClassification', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:28,557 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:28,557 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": \"MODEL_QUALITY\", \"problem_type\": \"BinaryClassification\", \"inference_attribute\": \"prediction\", \"probability_attribute\": \"probability\", \"ground_truth_attribute\": \"label\", \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:28,557 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:28,557 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:28,613 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:28,614 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:28,614 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'hosts': ['algo-1']}\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:28,623 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:28,623 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:28,623 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:29,527 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.138.184\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/\u001b[0m\n",
      "\u001b[34mhadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_392\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:29,538 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:29,542 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-550b9e8b-0b9c-4dcb-8a0c-66bee38919df\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,301 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,320 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,321 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,327 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,343 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,343 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,343 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,343 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,380 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,391 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,391 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,395 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,398 INFO blockmanagement.BlockManager: The block deletion will start around 2024 Mar 18 19:24:30\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,399 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,399 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,402 INFO util.GSet: 2.0% max memory 3.1 GB = 63.8 MB\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,402 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,436 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,440 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,440 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,440 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,440 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,440 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,440 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,440 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,440 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,440 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,440 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,440 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,468 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,468 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,468 INFO util.GSet: 1.0% max memory 3.1 GB = 31.9 MB\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,468 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,470 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,470 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,470 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,470 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,474 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,478 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,478 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,478 INFO util.GSet: 0.25% max memory 3.1 GB = 8.0 MB\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,478 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,515 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,515 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,515 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,518 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,518 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,520 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,520 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,520 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 979.8 KB\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,520 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,544 INFO namenode.FSImage: Allocated new BlockPoolId: BP-77931421-10.0.138.184-1710789870537\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,557 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,566 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,640 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,654 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,659 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.138.184\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:30,669 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:32,729 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:32,729 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:34,790 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:34,790 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:36,876 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:36,877 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:38,997 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:38,998 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:41,169 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:41,169 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:51,177 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:52,812 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:53,179 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:53,213 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:53,223 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:53,753 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:53,777 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:53,778 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:53,778 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:53,779 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:53,802 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11536, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:53,816 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:53,818 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:53,866 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:53,867 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:53,867 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:53,867 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:53,868 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:54,202 INFO util.Utils: Successfully started service 'sparkDriver' on port 36643.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:54,231 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:54,267 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:54,284 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:54,284 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:54,316 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:54,335 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-be9b9911-3dc1-4547-baee-b3c8d07fce26\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:54,355 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:54,392 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:54,429 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.138.184:36643/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1710789893749\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:54,935 INFO client.RMProxy: Connecting to ResourceManager at /10.0.138.184:8032\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:55,598 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:55,599 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:55,605 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15731 MB per container)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:55,605 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:55,605 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:55,606 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:55,612 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:55,697 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:57,427 INFO yarn.Client: Uploading resource file:/tmp/spark-0c18dc58-1e8a-40c1-858b-e85cadcb4f26/__spark_libs__1973367594011389840.zip -> hdfs://10.0.138.184/user/root/.sparkStaging/application_1710789876460_0001/__spark_libs__1973367594011389840.zip\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:58,967 INFO yarn.Client: Uploading resource file:/tmp/spark-0c18dc58-1e8a-40c1-858b-e85cadcb4f26/__spark_conf__8494198152665570124.zip -> hdfs://10.0.138.184/user/root/.sparkStaging/application_1710789876460_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:59,410 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:59,410 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:59,411 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:59,411 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:59,411 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:59,438 INFO yarn.Client: Submitting application application_1710789876460_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2024-03-18 19:24:59,644 INFO impl.YarnClientImpl: Submitted application application_1710789876460_0001\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:00,649 INFO yarn.Client: Application report for application_1710789876460_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:00,652 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Mon Mar 18 19:25:00 +0000 2024] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1710789899531\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1710789876460_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:01,656 INFO yarn.Client: Application report for application_1710789876460_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:02,658 INFO yarn.Client: Application report for application_1710789876460_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:03,662 INFO yarn.Client: Application report for application_1710789876460_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:04,666 INFO yarn.Client: Application report for application_1710789876460_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:05,046 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1710789876460_0001), /proxy/application_1710789876460_0001\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:05,669 INFO yarn.Client: Application report for application_1710789876460_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:05,669 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.138.184\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1710789899531\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1710789876460_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:05,671 INFO cluster.YarnClientSchedulerBackend: Application application_1710789876460_0001 has started running.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:05,678 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37135.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:05,678 INFO netty.NettyBlockTransferService: Server created on 10.0.138.184:37135\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:05,680 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:05,685 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.138.184, 37135, None)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:05,689 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.138.184:37135 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.138.184, 37135, None)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:05,692 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.138.184, 37135, None)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:05,693 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.138.184, 37135, None)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:05,795 INFO util.log: Logging initialized @14381ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:06,376 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:09,309 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.138.184:56962) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:09,490 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:43667 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 43667, None)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:24,845 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:24,938 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:24,972 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:24,974 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:25,791 INFO datasources.InMemoryFileIndex: It took 28 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:25,926 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:26,176 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:26,178 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.138.184:37135 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:26,181 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:26,488 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:26,490 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:26,492 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 757\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:26,533 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:26,548 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:26,548 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:26,549 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:26,550 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:26,555 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:26,597 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:26,601 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:26,602 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.138.184:37135 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:26,602 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:26,618 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:26,619 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:26,658 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4640 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:26,941 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:43667 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:27,788 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:43667 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:28,108 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1461 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:28,110 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:28,115 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.542 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:28,118 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:28,118 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:28,120 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.586735 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:28,274 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.138.184:37135 in memory (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:28,278 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:43667 in memory (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:28,300 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.138.184:37135 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:28,308 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:43667 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,305 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,306 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,308 INFO datasources.FileSourceStrategy: Output Data Schema: struct<>\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,812 INFO codegen.CodeGenerator: Code generated in 169.920891 ms\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,821 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,837 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,838 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.138.184:37135 (size: 39.1 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,839 INFO spark.SparkContext: Created broadcast 2 from count at BinaryClassificationAnalyzer.scala:81\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,851 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,902 INFO scheduler.DAGScheduler: Registering RDD 7 (count at BinaryClassificationAnalyzer.scala:81) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,906 INFO scheduler.DAGScheduler: Got map stage job 1 (count at BinaryClassificationAnalyzer.scala:81) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,907 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 1 (count at BinaryClassificationAnalyzer.scala:81)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,907 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,908 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,910 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at count at BinaryClassificationAnalyzer.scala:81), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,954 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.9 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,956 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,956 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.138.184:37135 (size: 8.1 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,957 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,958 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at count at BinaryClassificationAnalyzer.scala:81) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,958 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:30,962 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,015 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:43667 (size: 8.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,679 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:43667 (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,763 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 804 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,763 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,765 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (count at BinaryClassificationAnalyzer.scala:81) finished in 0.849 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,766 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,766 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,766 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,767 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,810 INFO codegen.CodeGenerator: Code generated in 13.977421 ms\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,844 INFO spark.SparkContext: Starting job: count at BinaryClassificationAnalyzer.scala:81\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,845 INFO scheduler.DAGScheduler: Got job 2 (count at BinaryClassificationAnalyzer.scala:81) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,845 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (count at BinaryClassificationAnalyzer.scala:81)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,845 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,846 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,847 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[10] at count at BinaryClassificationAnalyzer.scala:81), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,855 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 11.1 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,857 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,858 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.138.184:37135 (size: 5.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,859 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,860 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at count at BinaryClassificationAnalyzer.scala:81) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,860 INFO cluster.YarnScheduler: Adding task set 3.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,867 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,888 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:43667 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:31,917 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.138.184:56962\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,024 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 158 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,025 INFO scheduler.DAGScheduler: ResultStage 3 (count at BinaryClassificationAnalyzer.scala:81) finished in 0.172 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,026 INFO scheduler.DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,026 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,027 INFO cluster.YarnScheduler: Killing all running tasks in stage 3: Stage finished\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,027 INFO scheduler.DAGScheduler: Job 2 finished: count at BinaryClassificationAnalyzer.scala:81, took 0.182898 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,387 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,387 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,387 INFO datasources.FileSourceStrategy: Output Data Schema: struct<prediction: string, label: string>\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,415 INFO codegen.CodeGenerator: Code generated in 14.830979 ms\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,420 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,431 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,431 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.138.184:37135 (size: 39.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,432 INFO spark.SparkContext: Created broadcast 5 from rdd at ModelQualityAnalyzer.scala:311\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,433 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,479 INFO spark.SparkContext: Starting job: collectAsMap at ModelQualityAnalyzer.scala:313\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,482 INFO scheduler.DAGScheduler: Registering RDD 17 (rdd at ModelQualityAnalyzer.scala:311) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,483 INFO scheduler.DAGScheduler: Got job 3 (collectAsMap at ModelQualityAnalyzer.scala:313) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,483 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (collectAsMap at ModelQualityAnalyzer.scala:313)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,483 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,483 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 4)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,484 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[17] at rdd at ModelQualityAnalyzer.scala:311), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,512 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 28.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,513 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 13.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,513 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.138.184:37135 (size: 13.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,514 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,514 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[17] at rdd at ModelQualityAnalyzer.scala:311) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,514 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,516 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:32,534 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:43667 (size: 13.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,143 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:43667 (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,200 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 1685 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,200 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,203 INFO scheduler.DAGScheduler: ShuffleMapStage 4 (rdd at ModelQualityAnalyzer.scala:311) finished in 1.718 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,203 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,203 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,203 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 5)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,203 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,203 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (ShuffledRDD[18] at reduceByKey at ModelQualityAnalyzer.scala:313), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,206 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 5.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,208 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,208 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.138.184:37135 (size: 3.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,209 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,209 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (ShuffledRDD[18] at reduceByKey at ModelQualityAnalyzer.scala:313) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,209 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,211 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,225 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:43667 (size: 3.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,232 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.138.184:56962\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,258 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 47 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,258 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,259 INFO scheduler.DAGScheduler: ResultStage 5 (collectAsMap at ModelQualityAnalyzer.scala:313) finished in 0.055 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,260 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,260 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,260 INFO scheduler.DAGScheduler: Job 3 finished: collectAsMap at ModelQualityAnalyzer.scala:313, took 1.780406 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,334 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,334 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,335 INFO datasources.FileSourceStrategy: Output Data Schema: struct<prediction: string, label: string>\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,371 INFO codegen.CodeGenerator: Code generated in 28.15673 ms\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,377 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 416.5 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,391 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,392 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.138.184:37135 (size: 39.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,393 INFO spark.SparkContext: Created broadcast 8 from rdd at BinaryClassificationAnalyzer.scala:252\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,394 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,442 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,442 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,442 INFO datasources.FileSourceStrategy: Output Data Schema: struct<label: string>\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,463 INFO codegen.CodeGenerator: Code generated in 15.256649 ms\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,467 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 416.5 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,478 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,478 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.138.184:37135 (size: 39.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,479 INFO spark.SparkContext: Created broadcast 9 from rdd at BinaryClassificationAnalyzer.scala:256\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,480 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,512 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,512 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,512 INFO datasources.FileSourceStrategy: Output Data Schema: struct<probability: string, label: string>\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,533 INFO codegen.CodeGenerator: Code generated in 14.733431 ms\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,538 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 416.5 KiB, free 1456.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,553 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1456.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,554 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.138.184:37135 (size: 39.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,554 INFO spark.SparkContext: Created broadcast 10 from rdd at BinaryClassificationAnalyzer.scala:265\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,555 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,601 INFO spark.SparkContext: Starting job: count at BinaryClassificationMetrics.scala:197\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,603 INFO scheduler.DAGScheduler: Registering RDD 40 (map at BinaryClassificationMetrics.scala:48) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,603 INFO scheduler.DAGScheduler: Registering RDD 41 (combineByKey at BinaryClassificationMetrics.scala:188) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,603 INFO scheduler.DAGScheduler: Got job 4 (count at BinaryClassificationMetrics.scala:197) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,604 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (count at BinaryClassificationMetrics.scala:197)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,604 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,604 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,605 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[40] at map at BinaryClassificationMetrics.scala:48), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,613 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 34.2 KiB, free 1456.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,614 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 16.2 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,615 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.138.184:37135 (size: 16.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,615 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,616 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[40] at map at BinaryClassificationMetrics.scala:48) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,616 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,618 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,639 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:43667 (size: 16.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,843 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:43667 (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,903 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 286 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,904 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,905 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (map at BinaryClassificationMetrics.scala:48) finished in 0.299 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,906 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,907 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,907 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 7, ResultStage 8)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,907 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,908 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (ShuffledRDD[41] at combineByKey at BinaryClassificationMetrics.scala:188), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,916 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 5.7 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,917 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,918 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.138.184:37135 (size: 3.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,919 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,919 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (ShuffledRDD[41] at combineByKey at BinaryClassificationMetrics.scala:188) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,919 INFO cluster.YarnScheduler: Adding task set 7.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,921 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,932 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:43667 (size: 3.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,952 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.138.184:56962\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,965 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 45 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,965 INFO cluster.YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,966 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (combineByKey at BinaryClassificationMetrics.scala:188) finished in 0.057 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,966 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,966 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,966 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 8)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,966 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,968 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (ShuffledRDD[42] at sortByKey at BinaryClassificationMetrics.scala:189), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,972 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 5.3 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,974 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,974 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.138.184:37135 (size: 3.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,975 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,976 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ShuffledRDD[42] at sortByKey at BinaryClassificationMetrics.scala:189) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,976 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,978 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 7) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,987 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:43667 (size: 3.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:34,992 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.138.184:56962\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,014 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 7) in 37 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,014 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,015 INFO scheduler.DAGScheduler: ResultStage 8 (count at BinaryClassificationMetrics.scala:197) finished in 0.046 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,015 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,015 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,016 INFO scheduler.DAGScheduler: Job 4 finished: count at BinaryClassificationMetrics.scala:197, took 0.414036 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,017 INFO evaluation.BinaryClassificationMetrics: Curve is too small (6) for 1000 bins to be useful\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,031 INFO spark.SparkContext: Starting job: collect at BinaryClassificationMetrics.scala:237\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,033 INFO scheduler.DAGScheduler: Got job 5 (collect at BinaryClassificationMetrics.scala:237) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,033 INFO scheduler.DAGScheduler: Final stage: ResultStage 11 (collect at BinaryClassificationMetrics.scala:237)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,033 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,034 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,034 INFO scheduler.DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[44] at mapPartitions at BinaryClassificationMetrics.scala:237), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,039 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 6.7 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,041 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.7 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,041 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.138.184:37135 (size: 3.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,041 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,042 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[44] at mapPartitions at BinaryClassificationMetrics.scala:237) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,042 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,043 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,052 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:43667 (size: 3.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,076 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 8) in 32 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,076 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,076 INFO scheduler.DAGScheduler: ResultStage 11 (collect at BinaryClassificationMetrics.scala:237) finished in 0.039 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,077 INFO scheduler.DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,077 INFO cluster.YarnScheduler: Killing all running tasks in stage 11: Stage finished\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,077 INFO scheduler.DAGScheduler: Job 5 finished: collect at BinaryClassificationMetrics.scala:237, took 0.045888 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,080 INFO evaluation.BinaryClassificationMetrics: Total counts: {numPos: 55.0, numNeg: 36.0}\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,101 INFO spark.SparkContext: Starting job: collect at AreaUnderCurve.scala:44\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,102 INFO scheduler.DAGScheduler: Got job 6 (collect at AreaUnderCurve.scala:44) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,103 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (collect at AreaUnderCurve.scala:44)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,103 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,105 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,106 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[49] at mapPartitions at AreaUnderCurve.scala:44), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,109 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 8.3 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,110 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 4.1 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,111 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.138.184:37135 (size: 4.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,111 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,112 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[49] at mapPartitions at AreaUnderCurve.scala:44) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,112 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,114 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 9) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,130 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:43667 (size: 4.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,153 INFO storage.BlockManagerInfo: Added rdd_45_0 in memory on algo-1:43667 (size: 520.0 B, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,168 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 9) in 55 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,168 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,169 INFO scheduler.DAGScheduler: ResultStage 14 (collect at AreaUnderCurve.scala:44) finished in 0.062 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,170 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,170 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,170 INFO scheduler.DAGScheduler: Job 6 finished: collect at AreaUnderCurve.scala:44, took 0.069493 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,188 INFO spark.SparkContext: Starting job: first at BinaryClassificationMetrics.scala:135\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,190 INFO scheduler.DAGScheduler: Got job 7 (first at BinaryClassificationMetrics.scala:135) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,190 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (first at BinaryClassificationMetrics.scala:135)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,190 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,191 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,192 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[50] at map at BinaryClassificationMetrics.scala:272), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,195 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 7.6 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,196 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,197 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.138.184:37135 (size: 3.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,197 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,198 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[50] at map at BinaryClassificationMetrics.scala:272) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,198 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,200 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 10) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,211 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:43667 (size: 3.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,227 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 10) in 27 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,227 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,228 INFO scheduler.DAGScheduler: ResultStage 17 (first at BinaryClassificationMetrics.scala:135) finished in 0.034 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,228 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,228 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,228 INFO scheduler.DAGScheduler: Job 7 finished: first at BinaryClassificationMetrics.scala:135, took 0.039904 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,241 INFO spark.SparkContext: Starting job: collect at AreaUnderCurve.scala:44\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,242 INFO scheduler.DAGScheduler: Got job 8 (collect at AreaUnderCurve.scala:44) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,242 INFO scheduler.DAGScheduler: Final stage: ResultStage 20 (collect at AreaUnderCurve.scala:44)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,242 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,243 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,243 INFO scheduler.DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[52] at mapPartitions at AreaUnderCurve.scala:44), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,274 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 8.3 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,277 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.1 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,278 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.138.184:37135 (size: 4.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,279 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,279 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[52] at mapPartitions at AreaUnderCurve.scala:44) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,279 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,281 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,286 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.138.184:37135 in memory (size: 39.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,287 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:43667 in memory (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,301 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:43667 (size: 4.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,313 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 11) in 32 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,313 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,313 INFO scheduler.DAGScheduler: ResultStage 20 (collect at AreaUnderCurve.scala:44) finished in 0.040 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,314 INFO scheduler.DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,314 INFO cluster.YarnScheduler: Killing all running tasks in stage 20: Stage finished\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,314 INFO scheduler.DAGScheduler: Job 8 finished: collect at AreaUnderCurve.scala:44, took 0.073503 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,333 INFO spark.SparkContext: Starting job: collectAsMap at MulticlassMetrics.scala:61\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,334 INFO scheduler.DAGScheduler: Registering RDD 53 (map at MulticlassMetrics.scala:52) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,334 INFO scheduler.DAGScheduler: Got job 9 (collectAsMap at MulticlassMetrics.scala:61) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,334 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collectAsMap at MulticlassMetrics.scala:61)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,334 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,335 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,335 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[53] at map at MulticlassMetrics.scala:52), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,345 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.138.184:37135 in memory (size: 8.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,349 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:43667 in memory (size: 8.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,355 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 34.8 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,356 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,357 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.138.184:37135 (size: 16.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,358 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,358 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[53] at map at MulticlassMetrics.scala:52) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,358 INFO cluster.YarnScheduler: Adding task set 21.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,360 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 21.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,362 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.138.184:37135 in memory (size: 5.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,372 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:43667 in memory (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,373 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:43667 (size: 16.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,393 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.138.184:37135 in memory (size: 3.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,398 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:43667 in memory (size: 3.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,415 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.138.184:37135 in memory (size: 3.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,421 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:43667 in memory (size: 3.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,463 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:43667 (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,501 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:43667 in memory (size: 3.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,523 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.138.184:37135 in memory (size: 3.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,535 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 21.0 (TID 12) in 175 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,535 INFO cluster.YarnScheduler: Removed TaskSet 21.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,535 INFO scheduler.DAGScheduler: ShuffleMapStage 21 (map at MulticlassMetrics.scala:52) finished in 0.198 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,536 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,536 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,536 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 22)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,536 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,536 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (ShuffledRDD[54] at reduceByKey at MulticlassMetrics.scala:61), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,538 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 5.2 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,543 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,543 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.138.184:37135 (size: 3.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,544 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,544 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (ShuffledRDD[54] at reduceByKey at MulticlassMetrics.scala:61) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,544 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,546 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.138.184:37135 in memory (size: 3.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,546 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,562 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:43667 (size: 3.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,565 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.138.184:56962\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,572 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:43667 in memory (size: 3.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,579 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 13) in 33 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,579 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,580 INFO scheduler.DAGScheduler: ResultStage 22 (collectAsMap at MulticlassMetrics.scala:61) finished in 0.043 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,584 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,585 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,585 INFO scheduler.DAGScheduler: Job 9 finished: collectAsMap at MulticlassMetrics.scala:61, took 0.252579 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,594 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:43667 in memory (size: 16.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,594 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.138.184:37135 in memory (size: 16.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,610 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:43667 in memory (size: 4.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,611 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.138.184:37135 in memory (size: 4.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,616 INFO spark.SparkContext: Starting job: collectAsMap at MulticlassMetrics.scala:61\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,617 INFO scheduler.DAGScheduler: Registering RDD 55 (map at MulticlassMetrics.scala:52) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,620 INFO scheduler.DAGScheduler: Got job 10 (collectAsMap at MulticlassMetrics.scala:61) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,620 INFO scheduler.DAGScheduler: Final stage: ResultStage 24 (collectAsMap at MulticlassMetrics.scala:61)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,620 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,620 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 23)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,621 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[55] at map at MulticlassMetrics.scala:52), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,635 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 33.2 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,637 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,637 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.138.184:37135 (size: 15.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,642 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,643 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[55] at map at MulticlassMetrics.scala:52) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,643 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,644 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,652 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.138.184:37135 in memory (size: 3.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,653 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:43667 in memory (size: 3.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,753 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:43667 (size: 15.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,821 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:43667 (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,831 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.138.184:37135 in memory (size: 39.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,835 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-1:43667 in memory (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,856 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 14) in 211 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,856 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,858 INFO scheduler.DAGScheduler: ShuffleMapStage 23 (map at MulticlassMetrics.scala:52) finished in 0.236 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,858 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,858 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,858 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 24)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,858 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,858 INFO scheduler.DAGScheduler: Submitting ResultStage 24 (ShuffledRDD[56] at reduceByKey at MulticlassMetrics.scala:61), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,863 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 5.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,871 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,872 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:43667 in memory (size: 13.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,874 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.138.184:37135 (size: 3.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,874 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.138.184:37135 in memory (size: 13.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,875 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,876 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (ShuffledRDD[56] at reduceByKey at MulticlassMetrics.scala:61) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,876 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,877 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,895 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:43667 (size: 3.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,899 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.138.184:56962\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,911 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 15) in 34 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,912 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,912 INFO scheduler.DAGScheduler: ResultStage 24 (collectAsMap at MulticlassMetrics.scala:61) finished in 0.053 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,912 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,912 INFO cluster.YarnScheduler: Killing all running tasks in stage 24: Stage finished\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,913 INFO scheduler.DAGScheduler: Job 10 finished: collectAsMap at MulticlassMetrics.scala:61, took 0.296190 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,933 INFO spark.SparkContext: Starting job: collect at ModelQualityAnalyzer.scala:274\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,934 INFO scheduler.DAGScheduler: Got job 11 (collect at ModelQualityAnalyzer.scala:274) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,934 INFO scheduler.DAGScheduler: Final stage: ResultStage 27 (collect at ModelQualityAnalyzer.scala:274)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,934 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,935 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,935 INFO scheduler.DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[59] at map at ModelQualityAnalyzer.scala:274), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,938 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 8.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,940 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,941 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.138.184:37135 (size: 4.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,943 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,943 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[59] at map at ModelQualityAnalyzer.scala:274) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,943 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:35,945 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,272 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:43667 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,281 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 16) in 336 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,282 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,282 INFO scheduler.DAGScheduler: ResultStage 27 (collect at ModelQualityAnalyzer.scala:274) finished in 0.345 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,283 INFO scheduler.DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,283 INFO cluster.YarnScheduler: Killing all running tasks in stage 27: Stage finished\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,283 INFO scheduler.DAGScheduler: Job 11 finished: collect at ModelQualityAnalyzer.scala:274, took 0.350198 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,298 INFO spark.SparkContext: Starting job: collect at ModelQualityAnalyzer.scala:275\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,299 INFO scheduler.DAGScheduler: Got job 12 (collect at ModelQualityAnalyzer.scala:275) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,299 INFO scheduler.DAGScheduler: Final stage: ResultStage 30 (collect at ModelQualityAnalyzer.scala:275)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,299 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 29)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,299 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,300 INFO scheduler.DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[60] at map at ModelQualityAnalyzer.scala:275), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,302 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 8.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,304 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,304 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.138.184:37135 (size: 4.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,304 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,305 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[60] at map at ModelQualityAnalyzer.scala:275) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,305 INFO cluster.YarnScheduler: Adding task set 30.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,306 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 30.0 (TID 17) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,315 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:43667 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,321 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 30.0 (TID 17) in 15 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,321 INFO cluster.YarnScheduler: Removed TaskSet 30.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,322 INFO scheduler.DAGScheduler: ResultStage 30 (collect at ModelQualityAnalyzer.scala:275) finished in 0.021 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,322 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,322 INFO cluster.YarnScheduler: Killing all running tasks in stage 30: Stage finished\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,323 INFO scheduler.DAGScheduler: Job 12 finished: collect at ModelQualityAnalyzer.scala:275, took 0.025141 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,334 INFO spark.SparkContext: Starting job: first at BinaryClassificationMetrics.scala:135\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,335 INFO scheduler.DAGScheduler: Got job 13 (first at BinaryClassificationMetrics.scala:135) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,335 INFO scheduler.DAGScheduler: Final stage: ResultStage 33 (first at BinaryClassificationMetrics.scala:135)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,335 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,336 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,336 INFO scheduler.DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[61] at map at BinaryClassificationMetrics.scala:272), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,338 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 7.6 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,339 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,340 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.138.184:37135 (size: 3.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,340 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,341 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[61] at map at BinaryClassificationMetrics.scala:272) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,341 INFO cluster.YarnScheduler: Adding task set 33.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,342 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,350 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:43667 (size: 3.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,357 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 18) in 14 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,357 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,357 INFO scheduler.DAGScheduler: ResultStage 33 (first at BinaryClassificationMetrics.scala:135) finished in 0.020 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,357 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,357 INFO cluster.YarnScheduler: Killing all running tasks in stage 33: Stage finished\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,357 INFO scheduler.DAGScheduler: Job 13 finished: first at BinaryClassificationMetrics.scala:135, took 0.023439 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,367 INFO spark.SparkContext: Starting job: collect at ModelQualityAnalyzer.scala:274\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,368 INFO scheduler.DAGScheduler: Got job 14 (collect at ModelQualityAnalyzer.scala:274) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,368 INFO scheduler.DAGScheduler: Final stage: ResultStage 36 (collect at ModelQualityAnalyzer.scala:274)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,368 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 35)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,368 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,369 INFO scheduler.DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[63] at map at ModelQualityAnalyzer.scala:274), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,370 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 8.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,372 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,372 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.138.184:37135 (size: 4.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,372 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,374 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[63] at map at ModelQualityAnalyzer.scala:274) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,374 INFO cluster.YarnScheduler: Adding task set 36.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,375 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,383 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:43667 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,388 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 19) in 13 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,388 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,389 INFO scheduler.DAGScheduler: ResultStage 36 (collect at ModelQualityAnalyzer.scala:274) finished in 0.019 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,389 INFO scheduler.DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,389 INFO cluster.YarnScheduler: Killing all running tasks in stage 36: Stage finished\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,389 INFO scheduler.DAGScheduler: Job 14 finished: collect at ModelQualityAnalyzer.scala:274, took 0.022083 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,396 INFO spark.SparkContext: Starting job: collect at ModelQualityAnalyzer.scala:275\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,397 INFO scheduler.DAGScheduler: Got job 15 (collect at ModelQualityAnalyzer.scala:275) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,397 INFO scheduler.DAGScheduler: Final stage: ResultStage 39 (collect at ModelQualityAnalyzer.scala:275)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,397 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 38)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,397 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,398 INFO scheduler.DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[64] at map at ModelQualityAnalyzer.scala:275), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,399 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 8.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,400 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,401 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.138.184:37135 (size: 4.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,401 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,401 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[64] at map at ModelQualityAnalyzer.scala:275) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,401 INFO cluster.YarnScheduler: Adding task set 39.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,403 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 39.0 (TID 20) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,412 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:43667 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,417 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 39.0 (TID 20) in 15 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,418 INFO cluster.YarnScheduler: Removed TaskSet 39.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,418 INFO scheduler.DAGScheduler: ResultStage 39 (collect at ModelQualityAnalyzer.scala:275) finished in 0.020 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,419 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,419 INFO cluster.YarnScheduler: Killing all running tasks in stage 39: Stage finished\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,419 INFO scheduler.DAGScheduler: Job 15 finished: collect at ModelQualityAnalyzer.scala:275, took 0.023126 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,441 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,441 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,441 INFO datasources.FileSourceStrategy: Output Data Schema: struct<>\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,460 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 416.5 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,475 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,475 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.138.184:37135 (size: 39.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,476 INFO spark.SparkContext: Created broadcast 27 from count at ModelQualityAnalyzer.scala:144\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,477 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,482 INFO scheduler.DAGScheduler: Registering RDD 68 (count at ModelQualityAnalyzer.scala:144) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,482 INFO scheduler.DAGScheduler: Got map stage job 16 (count at ModelQualityAnalyzer.scala:144) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,482 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 40 (count at ModelQualityAnalyzer.scala:144)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,482 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,482 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,483 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 40 (MapPartitionsRDD[68] at count at ModelQualityAnalyzer.scala:144), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,485 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 15.9 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,486 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,487 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.138.184:37135 (size: 8.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,487 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,488 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[68] at count at ModelQualityAnalyzer.scala:144) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,488 INFO cluster.YarnScheduler: Adding task set 40.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,489 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,498 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:43667 (size: 8.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,514 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:43667 (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,532 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 21) in 43 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,532 INFO cluster.YarnScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,533 INFO scheduler.DAGScheduler: ShuffleMapStage 40 (count at ModelQualityAnalyzer.scala:144) finished in 0.050 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,534 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,534 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,534 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,534 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,560 INFO spark.SparkContext: Starting job: count at ModelQualityAnalyzer.scala:144\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,560 INFO scheduler.DAGScheduler: Got job 17 (count at ModelQualityAnalyzer.scala:144) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,561 INFO scheduler.DAGScheduler: Final stage: ResultStage 42 (count at ModelQualityAnalyzer.scala:144)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,561 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 41)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,561 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,561 INFO scheduler.DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[71] at count at ModelQualityAnalyzer.scala:144), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,570 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 11.1 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,572 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,574 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.138.184:37135 (size: 5.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,574 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,574 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[71] at count at ModelQualityAnalyzer.scala:144) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,574 INFO cluster.YarnScheduler: Adding task set 42.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,576 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 42.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,585 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:43667 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,591 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.138.184:56962\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,598 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 42.0 (TID 22) in 22 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,598 INFO cluster.YarnScheduler: Removed TaskSet 42.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,599 INFO scheduler.DAGScheduler: ResultStage 42 (count at ModelQualityAnalyzer.scala:144) finished in 0.030 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,600 INFO scheduler.DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,600 INFO cluster.YarnScheduler: Killing all running tasks in stage 42: Stage finished\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,601 INFO scheduler.DAGScheduler: Job 17 finished: count at ModelQualityAnalyzer.scala:144, took 0.040796 s\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,734 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,751 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,760 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,775 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,776 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,782 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,794 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,824 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,826 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,838 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,841 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,885 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,885 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,885 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,893 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,893 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-c7962b29-f42d-429b-8bac-b0a9aafd6cf4\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,900 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-0c18dc58-1e8a-40c1-858b-e85cadcb4f26\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,949 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2024-03-18 19:25:36,949 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job = model_quality_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_dataset_uri,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    problem_type=\"BinaryClassification\",\n",
    "    inference_attribute=\"prediction\",\n",
    "    probability_attribute=\"probability\",\n",
    "    ground_truth_attribute=\"label\",\n",
    ")\n",
    "job.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44b97fbe-1b2f-4d7a-b29b-c594d9b7e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpointInput = EndpointInput(\n",
    "    endpoint_name=endpoint_name,\n",
    "    probability_attribute=\"0\",\n",
    "    probability_threshold_attribute=0.5,\n",
    "    destination=\"/opt/ml/processing/input_data\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f538b790-444a-4982-abb0-ea1dddf97d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-scikit-learn-2024-03-21-00-03-05-484'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a860b8c9-170b-4870-92d3-1b45dfe25456",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model_quality_monitor.create_monitoring_schedule(\n",
    "    endpoint_input=endpointInput,\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    problem_type=\"BinaryClassification\",\n",
    "    ground_truth_input=\"s3://sagemaker-us-east-1-381492271173/monitor/heart/ground-truth\",\n",
    "    constraints=\"s3://sagemaker-us-east-1-381492271173/monitor/heart/baseline-results/constraints.json\",\n",
    "    schedule_cron_expression=CronExpressionGenerator.now(),\n",
    "    data_analysis_start_time=\"-PT1H\",\n",
    "    data_analysis_end_time=\"-PT0H\",\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "383a6620-479d-429a-ae38-8e0be74e5d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MonitoringScheduleArn': 'arn:aws:sagemaker:us-east-1:381492271173:monitoring-schedule/monitoring-schedule-2024-03-21-01-19-31-499',\n",
       " 'MonitoringScheduleName': 'monitoring-schedule-2024-03-21-01-19-31-499',\n",
       " 'MonitoringScheduleStatus': 'Pending',\n",
       " 'MonitoringType': 'ModelQuality',\n",
       " 'CreationTime': datetime.datetime(2024, 3, 21, 1, 19, 32, 302000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2024, 3, 21, 1, 19, 32, 395000, tzinfo=tzlocal()),\n",
       " 'MonitoringScheduleConfig': {'ScheduleConfig': {'ScheduleExpression': 'NOW',\n",
       "   'DataAnalysisStartTime': '-PT1H',\n",
       "   'DataAnalysisEndTime': '-PT0H'},\n",
       "  'MonitoringJobDefinitionName': 'model-quality-job-definition-2024-03-21-01-19-31-499',\n",
       "  'MonitoringType': 'ModelQuality'},\n",
       " 'EndpointName': 'sagemaker-scikit-learn-2024-03-21-00-03-05-484',\n",
       " 'ResponseMetadata': {'RequestId': '1f602727-62e3-4d01-a872-ff411d4ab47b',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '1f602727-62e3-4d01-a872-ff411d4ab47b',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '658',\n",
       "   'date': 'Thu, 21 Mar 2024 01:19:42 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quality_monitor.describe_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "626f4925-5802-440a-8a16-84fd627df466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No executions found for schedule. monitoring_schedule_name: monitoring-schedule-2024-03-20-01-49-25-076\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_quality_monitor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_executions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "model_quality_monitor.list_executions()[-1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ba0f4aa7-7cba-40b1-9919-e3dfad1b8703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleting Monitoring Schedule with name: monitoring-schedule-2024-03-21-01-19-31-499\n"
     ]
    }
   ],
   "source": [
    "#model_quality_monitor.delete_monitoring_schedule()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
